{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18e395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from openood.evaluation_api import Evaluator\n",
    "from openood.networks import ResNet18_32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cee6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0f122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesize = 32\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((imagesize, imagesize)),\n",
    "    transforms.CenterCrop(imagesize),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "    # transforms.Normalize([x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "    #                     [x/255.0 for x in [63.0, 62.1, 66.7]]),\n",
    "])\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # transforms.RandomCrop(imagesize, padding=4),\n",
    "    transforms.RandomResizedCrop(size=imagesize, scale=(0.2, 1.)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "    # transforms.Normalize([x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "    #                      [x / 255.0 for x in [63.0, 62.1, 66.7]]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21fdea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir4 = './data/images_classic/cifar4/cifar4/train'\n",
    "test_dir4 = './data/images_classic/cifar4/cifar4/test'\n",
    "train_dir6 = './data/images_classic/cifar6/cifar6/train'\n",
    "test_dir6 = './data/images_classic/cifar6/cifar6/test'\n",
    "train_dir10 = './data/images_classic/cifar10/cifar10/train'\n",
    "test_dir10 = './data/images_classic/cifar10/cifar10/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b800bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir10 = './data/images_classic/cifar10/cifar10/train'\n",
    "test_dir10 = './data/images_classic/cifar10/cifar10/test'\n",
    "train_dir100 = './data/images_classic/cifar100/cifar100/train'\n",
    "test_dir100 = './data/images_classic/cifar100/cifar100/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "068f58e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhao\\AppData\\Local\\Temp\\ipykernel_50676\\576630599.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load('resnet_cifar4.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet18_32x32(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResNet18_32x32(num_classes=10)\n",
    "net.load_state_dict(torch.load('resnet_cifar4.pth'))\n",
    "net.cuda()\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "225cefb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]),\n",
    "])\n",
    "transform_cifar100 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5071, 0.4867, 0.4408], [0.2675, 0.2565, 0.2761]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb345b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,dataloaders):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables to track accuracy per class\n",
    "    correct_preds = {classname: 0 for classname in class_names}\n",
    "    total_preds = {classname: 0 for classname in class_names}\n",
    "\n",
    "    # Evaluation loop\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders['test']:\n",
    "            inputs = inputs.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Track accuracy for each class\n",
    "            for label, pred in zip(labels, preds):\n",
    "                if pred == label:\n",
    "                    correct_preds[class_names[label]] += 1\n",
    "                total_preds[class_names[label]] += 1\n",
    "\n",
    "    # Calculate and print accuracy for each class\n",
    "    for classname, correct_count in correct_preds.items():\n",
    "        accuracy = 100 * float(correct_count) / total_preds[classname]\n",
    "        print(f'Accuracy for class {classname}: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "888232c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir4 = './data/images_classic/cifar4/cifar4/train'\n",
    "test_dir4 = './data/images_classic/cifar4/cifar4/test'\n",
    "train_dir6b = './data/images_classic/cifar6b/cifar6b/train'\n",
    "test_dir6b = './data/images_classic/cifar6b/cifar6b/test'\n",
    "train_dir10 = './data/images_classic/cifar10/cifar10/train'\n",
    "test_dir10 = './data/images_classic/cifar10/cifar10/test'\n",
    "train_dir100 = './data/images_classic/cifar100/cifar100/train'\n",
    "test_dir100 = './data/images_classic/cifar100/cifar100/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d56ec1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test datasets\n",
    "image_datasets4 = {\n",
    "    'train': datasets.ImageFolder(train_dir4, transform=transform_cifar10),\n",
    "    'test': datasets.ImageFolder(test_dir4, transform=transform_cifar10)\n",
    "}\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloaders4 = {\n",
    "    'train': DataLoader(image_datasets4['train'], batch_size=64, shuffle=True, num_workers=4),\n",
    "    'test': DataLoader(image_datasets4['test'], batch_size=64, shuffle=False, num_workers=4)\n",
    "}\n",
    "# Load train and test datasets\n",
    "image_datasets6b = {\n",
    "    'train': datasets.ImageFolder(train_dir6b, transform=transform_cifar10),\n",
    "    'test': datasets.ImageFolder(test_dir6b, transform=transform_cifar10)\n",
    "}\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloaders6b = {\n",
    "    'train': DataLoader(image_datasets6b['train'], batch_size=64, shuffle=True, num_workers=4),\n",
    "    'test': DataLoader(image_datasets6b['test'], batch_size=64, shuffle=False, num_workers=4)\n",
    "}\n",
    "# Load train and test datasets\n",
    "image_datasets10 = {\n",
    "    'train': datasets.ImageFolder(train_dir10, transform=transform_cifar10),\n",
    "    'test': datasets.ImageFolder(test_dir10, transform=transform_cifar10)\n",
    "}\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloaders10 = {\n",
    "    'train': DataLoader(image_datasets10['train'], batch_size=64, shuffle=True, num_workers=4),\n",
    "    'test': DataLoader(image_datasets10['test'], batch_size=64, shuffle=False, num_workers=4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a07b8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names in CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e815774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class airplane: 90.40%\n",
      "Accuracy for class automobile: 99.50%\n",
      "Accuracy for class bird: 85.30%\n",
      "Accuracy for class cat: 80.40%\n",
      "Accuracy for class deer: 0.00%\n",
      "Accuracy for class dog: 0.00%\n",
      "Accuracy for class frog: 0.00%\n",
      "Accuracy for class horse: 0.00%\n",
      "Accuracy for class ship: 0.00%\n",
      "Accuracy for class truck: 0.00%\n"
     ]
    }
   ],
   "source": [
    "a=evaluate(net,dataloaders10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476db83d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db1651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93594b26",
   "metadata": {},
   "source": [
    "## feat_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "250c69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "device = 'cuda' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a136edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bde47fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21fad16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizer function to ensure unit norm\n",
    "normalizer = lambda x: x / np.linalg.norm(x, axis=-1, keepdims=True) + 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f5f5cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNPostprocessor():\n",
    "    def __init__(self, K):\n",
    "        self.K = K\n",
    "        self.activation_log = None\n",
    "\n",
    "    def setup(self, net: nn.Module, id_loader_dict, ood_loader_dict):\n",
    "            activation_log = []\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(id_loader_dict['train'],\n",
    "                                  desc='Setup: ',\n",
    "                                  position=0,\n",
    "                                  leave=True):\n",
    "                    data = batch[0].cuda()\n",
    "                    data = data.float()\n",
    "\n",
    "                    _, feature = net(data, return_feature=True)\n",
    "                    activation_log.append(\n",
    "                        normalizer(feature.data.cpu().numpy()))\n",
    "\n",
    "            self.activation_log = np.concatenate(activation_log, axis=0)\n",
    "            self.index = faiss.IndexFlatL2(feature.shape[1])\n",
    "            self.index.add(self.activation_log)\n",
    "    '''    def postprocess(self, net: nn.Module, data):\n",
    "        output, feature = net(data, return_feature=True)\n",
    "        feature_normed = normalizer(feature.data.cpu().numpy())\n",
    "        D, _ = self.index.search(\n",
    "            feature_normed,\n",
    "            self.K,\n",
    "        )\n",
    "        kth_dist = -D[:, -1]\n",
    "        _, pred = torch.max(torch.softmax(output, dim=1), dim=1)\n",
    "        return pred, torch.from_numpy(kth_dist)'''\n",
    "    def detect_ood(self, net: nn.Module, ood_loader):\n",
    "        \"\"\"\n",
    "        Perform KNN-based OOD detection by computing distances to the nearest neighbors.\n",
    "        \n",
    "        Args:\n",
    "        - net (nn.Module): The neural network model used for feature extraction.\n",
    "        - ood_loader (DataLoader): DataLoader for the out-of-distribution (OOD) dataset (test data).\n",
    "\n",
    "        Returns:\n",
    "        - scores (torch.Tensor): OOD scores based on K-th nearest neighbor distances.\n",
    "        \"\"\"\n",
    "        ood_scores = []\n",
    "        net.eval()\n",
    "\n",
    "        # Extract features for OOD samples and compute distances to nearest neighbors\n",
    "        for batch in tqdm(ood_loader, desc='Processing OOD data', position=0, leave=True):\n",
    "            data = batch[0].cuda()  # Get the input data and move to GPU\n",
    "            _, feature = net(data, return_feature=True)  # Extract features\n",
    "\n",
    "            # Normalize the features\n",
    "            feature_normed = normalizer(feature.data.cpu().numpy())\n",
    "\n",
    "            # Search K nearest neighbors and get distances\n",
    "            D, _ = self.index.search(feature_normed, self.K)\n",
    "            \n",
    "            # Use the K-th distance as the OOD score\n",
    "            kth_dist = -D[:, -1]  # Negative distance for consistency with OOD scoring\n",
    "            ood_scores.append(torch.from_numpy(kth_dist))\n",
    "\n",
    "        # Concatenate all OOD scores\n",
    "        ood_scores = torch.cat(ood_scores, dim=0)\n",
    "        return ood_scores\n",
    "    def evaluate(self, id_loader, ood_loader, net):\n",
    "        \"\"\"\n",
    "        Evaluate OOD detection performance by comparing ID and OOD samples.\n",
    "        \n",
    "        Args:\n",
    "        - id_loader (DataLoader): DataLoader for the in-distribution dataset (test data).\n",
    "        - ood_loader (DataLoader): DataLoader for the out-of-distribution dataset (test data).\n",
    "        - net (nn.Module): The neural network model used for feature extraction.\n",
    "\n",
    "        Returns:\n",
    "        - fpr_at_95_tpr (float): False positive rate at 95% true positive rate.\n",
    "        \"\"\"\n",
    "        # Extract ID and OOD features\n",
    "        self.id_scores = self.detect_ood(net, id_loader)\n",
    "        self.ood_scores = self.detect_ood(net, ood_loader)\n",
    "\n",
    "        # Calculate FPR at 95% TPR\n",
    "        fpr_at_95_tpr = self.calculate_fpr_at_95_tpr(self.id_scores, self.ood_scores)\n",
    "        return fpr_at_95_tpr\n",
    "    def calculate_fpr_at_95_tpr(self,id_scores, ood_scores):\n",
    "        \"\"\"\n",
    "        Calculate FPR at 95% TPR for OOD detection.\n",
    "\n",
    "        Args:\n",
    "        - id_scores (torch.Tensor): Scores for in-distribution samples.\n",
    "        - ood_scores (torch.Tensor): Scores for out-of-distribution samples.\n",
    "\n",
    "        Returns:\n",
    "        - fpr_at_95_tpr (float): False positive rate at 95% true positive rate.\n",
    "        \"\"\"\n",
    "        labels = np.concatenate([np.zeros_like(id_scores), np.ones_like(ood_scores)])\n",
    "        scores = np.concatenate([id_scores, ood_scores])\n",
    "\n",
    "        # Sort scores and labels based on the score threshold\n",
    "        sorted_indices = np.argsort(scores)\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "\n",
    "        # Calculate TPR and FPR\n",
    "        tpr = np.cumsum(sorted_labels) / np.sum(sorted_labels)\n",
    "        fpr = np.cumsum(1 - sorted_labels) / np.sum(1 - sorted_labels)\n",
    "\n",
    "        # Find FPR where TPR is closest to 95%\n",
    "        idx = np.searchsorted(tpr, 0.95)\n",
    "        fpr_at_95_tpr = fpr[idx]\n",
    "        return fpr_at_95_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f96bb59b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f51e155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setup: 100%|█████████████████████████████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.18it/s]\n",
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 63/63 [00:36<00:00,  1.72it/s]\n",
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 94/94 [00:50<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR at 95% TPR: 0.7535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k50 = KNNPostprocessor(50)\n",
    "# Setup Faiss index with in-distribution training data\n",
    "k50.setup(net, dataloaders4,dataloaders6b )\n",
    "# Evaluate OOD detection performance\n",
    "fpr_at_95_tpr = k50.evaluate(dataloaders4['test'], dataloaders6b['test'], net)\n",
    "print(f\"FPR at 95% TPR: {fpr_at_95_tpr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc0d358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setup: 100%|█████████████████████████████████████████████████████████████████████████| 313/313 [00:09<00:00, 33.81it/s]\n",
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 63/63 [00:36<00:00,  1.72it/s]\n",
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 94/94 [00:51<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR at 95% TPR: 0.7538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k10 = KNNPostprocessor(10)\n",
    "# Setup Faiss index with in-distribution training data\n",
    "k10.setup(net, dataloaders4,dataloaders6b )\n",
    "# Evaluate OOD detection performance\n",
    "fpr_at_95_tpr = k10.evaluate(dataloaders4['test'], dataloaders6b['test'], net)\n",
    "print(f\"FPR at 95% TPR: {fpr_at_95_tpr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b200057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setup: 100%|█████████████████████████████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.29it/s]\n",
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 63/63 [00:37<00:00,  1.67it/s]\n",
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 94/94 [00:50<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR at 95% TPR: 0.7538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k100 = KNNPostprocessor(100)\n",
    "# Setup Faiss index with in-distribution training data\n",
    "k100.setup(net, dataloaders4,dataloaders6b )\n",
    "# Evaluate OOD detection performance\n",
    "fpr_at_95_tpr = k10.evaluate(dataloaders4['test'], dataloaders6b['test'], net)\n",
    "print(f\"FPR at 95% TPR: {fpr_at_95_tpr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "732efe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setup: 100%|█████████████████████████████████████████████████████████████████████████| 313/313 [00:09<00:00, 34.54it/s]\n"
     ]
    }
   ],
   "source": [
    "k50 = KNNPostprocessor(50)\n",
    "k50.setup(net, dataloaders4,dataloaders6b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab292564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 63/63 [00:38<00:00,  1.66it/s]\n",
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 16/16 [00:14<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR at 95% TPR for class 'deer': 0.6143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 63/63 [00:35<00:00,  1.77it/s]\n",
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 16/16 [00:14<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR at 95% TPR for class 'dog': 0.6127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 63/63 [00:36<00:00,  1.73it/s]\n",
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR at 95% TPR for class 'frog': 0.6058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 63/63 [00:36<00:00,  1.72it/s]\n",
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 16/16 [00:14<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR at 95% TPR for class 'horse': 0.4608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 63/63 [00:35<00:00,  1.77it/s]\n",
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 16/16 [00:15<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR at 95% TPR for class 'ship': 0.9575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 63/63 [00:36<00:00,  1.72it/s]\n",
      "Processing OOD data: 100%|█████████████████████████████████████████████████████████████| 16/16 [00:14<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR at 95% TPR for class 'truck': 0.8045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classes = ('deer', 'dog', 'frog', 'horse', 'ship', 'truck') # Adjust based on actual classes in dataset6b\n",
    "\n",
    "# Loop through each class in dataset6b\n",
    "for i, cls_name in enumerate(classes):\n",
    "    # Filter test data to include only the current class for OOD evaluation\n",
    "    class_indices = [idx for idx, (_, label) in enumerate(image_datasets6b['test'].imgs) if label == i]\n",
    "    class_subset = torch.utils.data.Subset(image_datasets6b['test'], class_indices)\n",
    "    class_loader = DataLoader(class_subset, batch_size=64, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Evaluate OOD detection performance for the current class\n",
    "    fpr_at_95_tpr = k50.evaluate(dataloaders4['test'], class_loader, net)\n",
    "    print(f\"FPR at 95% TPR for class '{cls_name}': {fpr_at_95_tpr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8a9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c451ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7786cac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6cfcf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123201b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b76159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b9e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Openood",
   "language": "python",
   "name": "openood"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
